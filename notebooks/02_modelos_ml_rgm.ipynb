{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Modelos de Machine Learning para RGM\n",
    "\n",
    "## Objetivo\n",
    "Implementar modelos de ML y Deep Learning para:\n",
    "- **Elasticidad de Precios**: An√°lisis de sensibilidad precio-demanda\n",
    "- **Segmentaci√≥n Avanzada**: Clustering inteligente de clientes\n",
    "- **Predicci√≥n de CLV**: Modelos predictivos de valor de cliente\n",
    "- **Optimizaci√≥n de Promociones**: ML para ROI promocional\n",
    "- **Predicci√≥n de Demanda**: Deep Learning para forecasting\n",
    "- **Recomendaci√≥n de Precios**: Neural Networks para pricing √≥ptimo\n",
    "\n",
    "## Metodolog√≠a RGM + ML\n",
    "1. **Price Elasticity Modeling** - Regresi√≥n y modelos econom√©tricos\n",
    "2. **Customer Segmentation** - K-means, DBSCAN, Gaussian Mixture\n",
    "3. **CLV Prediction** - Random Forest, XGBoost, Neural Networks\n",
    "4. **Promotion Optimization** - Reinforcement Learning\n",
    "5. **Demand Forecasting** - LSTM, GRU, Transformer\n",
    "6. **Dynamic Pricing** - Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow/Keras disponible\n",
      "‚úÖ Statsmodels disponible\n",
      "üöÄ Librer√≠as ML/DL cargadas exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Librer√≠as b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "#from sklearn.cluster import KMeans, DBSCAN, GaussianMixture\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    print('‚úÖ TensorFlow/Keras disponible')\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è TensorFlow no disponible - usando solo sklearn')\n",
    "\n",
    "# Estad√≠stica y econometr√≠a\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    print('‚úÖ Statsmodels disponible')\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è Statsmodels no disponible')\n",
    "\n",
    "print('üöÄ Librer√≠as ML/DL cargadas exitosamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Preparaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Cargando datasets para ML...\n",
      "üîÑ Limpiando formato num√©rico...\n",
      "Transacciones: 4,090,711\n",
      "Clientes: 21,269\n",
      "Productos: 1,600\n",
      "Promociones: 2,908\n",
      "\n",
      "üìã Columnas disponibles:\n",
      "Ventas: ['√Ø¬ª¬ø\"ORDEN\"', 'POSICION', 'SECUENCIA', 'ID_CLIENTE', 'ARTICULO', 'CNTR', 'SKU', 'ID_ALMACEN', 'TIPO_TRANSACCION', 'NRO_FACTURA', 'FECHA_ORDEN_FECHA', 'FECHA_ORDEN_HORA', 'FECHA_ENTREGA_FECHA', 'FECHA_ENTREGA_HORA', 'FECHA_FACTURA_FECHA', 'FECHA_FACTURA_HORA', 'QTY_PEDIDA', 'QTY_ENTREGADA', 'QTY_SUGERIDA', 'QTY_RETRO_ORDEN', 'STATUS_VTA', 'UND_PRECIO_VTA', 'FACTOR_PRECIO_UND_VTA', 'ID_FAMILIA_ORDEN', 'UND_VENTA', 'FACTOR_UND_VTA', 'ID_CONDICIONPAGO', 'ID_TIPOORDEN', 'TIPO_ORDEN', 'ID_VENDEDOR', 'ID_CATEGORIA_VDI', 'CLIENTE_ANEXO', 'SOURCE_TBL', 'ANIO', 'PERIODO', 'ID_PROMOCION', 'NOMBRE_PROMOCION', 'CTA_CTBLE', 'PEDIDO_MTO', 'TIPO_LINEA', 'UND_X_CAJA', 'ID_UNDSTOCK', 'QTY_KILOS', 'PRECIO_COSTOSTD', 'PRECIO', 'PCTJ_DSCTO', 'IMPORTE_DSCTO', 'IMPORTE', 'IMPORTE_DSCTO_LINEA_ORDEN', 'IMPORTE_DSCTO_ORDEN', 'VALOR_DSCTO_ADICIONAL', 'PCTJ_COMPENSACION_IVA', 'VALOR_DSCTO_CONTABLE', 'VALOR_DSCTO_FINANCIERO', 'VALOR_VARIACION', 'VALOR_CTV', 'VALOR_GO', 'PAISDESTINO', 'KG_PEND', 'VARIACION_SWAP', 'VARIACION_SWAPDIR', 'VARIACION_ESFIN', 'VARIACION_INVENTARIO', 'VALOR_REGALIAS', 'VARIACION_LOTES', 'VALOR_REGMKT', 'VAR_MANO_OBRA', 'VAR_COSTO_DIRECTO', 'VAR_GASTO_GRAL_FABRIC', 'COSTO_MANO_OBRA', 'COSTO_DIRECTO', 'GASTO_GRAL_FABRIC', 'VALOR_VARIACIONNEW', 'MPRIMA', 'PTCOMPRADO', 'QUIMINGR', 'MEMPAQUE', 'MINDIRECTO', 'COSTOSERV', 'FLETE_PRIMARIO', 'DESCCTAVARIACION', 'VARIACION_PRECIOS', 'VARIACION_CONSUMO', 'VARIACION_RECUPERACION', 'INDICADOR', 'FLETE', 'ARTICULO1', 'TIPO']\n",
      "Clientes: ['ID_CLIENTE', 'Venta_Total', 'Ticket_Promedio', 'Num_Compras', 'Cantidad_Total', 'Primera_Compra', 'Ultima_Compra', 'Segmento', 'CLV_Estimado']\n",
      "Productos: ['SKU', 'Articulo', 'Cantidad_Total', 'Venta_Total', 'Precio_Promedio', 'Costo_Promedio', 'Num_Transacciones', 'Margen_Unitario', 'Margen_Porcentaje', 'Venta_Acumulada', 'Porcentaje_Acumulado', 'Clasificacion_ABC']\n",
      "\n",
      "üîç Verificaci√≥n de datos num√©ricos:\n",
      "PRECIO: min=0.0, max=2160.0, nulls=0\n",
      "QTY_ENTREGADA: min=0.0, max=55740.0, nulls=0\n",
      "IMPORTE: min=0.0, max=101438.06, nulls=0\n"
     ]
    }
   ],
   "source": [
    "# Definir rutas\n",
    "DATA_PATH = '../data/processed/'\n",
    "\n",
    "# Funci√≥n para limpiar formato num√©rico con comas decimales\n",
    "def clean_numeric_columns(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Convierte columnas num√©ricas que usan coma como separador decimal\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    for col in numeric_cols:\n",
    "        if col in df_clean.columns:\n",
    "            # Convertir a string primero para manejar valores no string\n",
    "            df_clean[col] = df_clean[col].astype(str)\n",
    "            # Reemplazar comas por puntos decimales\n",
    "            df_clean[col] = df_clean[col].str.replace(',', '.', regex=False)\n",
    "            # Manejar valores vac√≠os o no num√©ricos\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    return df_clean\n",
    "\n",
    "# Cargar datasets\n",
    "print('üìä Cargando datasets para ML...')\n",
    "\n",
    "# Cargar datos principales\n",
    "df_ventas = pd.read_csv(DATA_PATH + 'datos_limpios.csv')\n",
    "\n",
    "# Identificar columnas num√©ricas que pueden tener formato con comas\n",
    "numeric_columns = ['PRECIO', 'QTY_ENTREGADA', 'QTY_PEDIDA', 'QTY_SUGERIDA', 'QTY_KILOS', \n",
    "                  'IMPORTE', 'IMPORTE_DSCTO', 'PRECIO_COSTOSTD', 'PCTJ_DSCTO']\n",
    "\n",
    "# Limpiar formato num√©rico\n",
    "print('üîÑ Limpiando formato num√©rico...')\n",
    "df_ventas = clean_numeric_columns(df_ventas, numeric_columns)\n",
    "\n",
    "# Cargar otros datasets\n",
    "df_clientes = pd.read_csv(DATA_PATH + 'maestro_clientes_rgm.csv')\n",
    "df_productos = pd.read_csv(DATA_PATH + 'maestro_productos_rgm.csv')\n",
    "df_promociones = pd.read_csv(DATA_PATH + 'analisis_promociones_rgm.csv')\n",
    "\n",
    "print(f'Transacciones: {df_ventas.shape[0]:,}')\n",
    "print(f'Clientes: {df_clientes.shape[0]:,}')\n",
    "print(f'Productos: {df_productos.shape[0]:,}')\n",
    "print(f'Promociones: {df_promociones.shape[0]:,}')\n",
    "\n",
    "# Verificar columnas de datos principales\n",
    "print('\\nüìã Columnas disponibles:')\n",
    "print('Ventas:', list(df_ventas.columns))\n",
    "print('Clientes:', list(df_clientes.columns))\n",
    "print('Productos:', list(df_productos.columns))\n",
    "\n",
    "# Verificar que las columnas num√©ricas se cargaron correctamente\n",
    "print('\\nüîç Verificaci√≥n de datos num√©ricos:')\n",
    "for col in ['PRECIO', 'QTY_ENTREGADA', 'IMPORTE']:\n",
    "    if col in df_ventas.columns:\n",
    "        print(f'{col}: min={df_ventas[col].min()}, max={df_ventas[col].max()}, nulls={df_ventas[col].isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo de Elasticidad de Precios üìà\n",
    "\n",
    "### An√°lisis econom√©trico de la relaci√≥n precio-demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà AN√ÅLISIS DE ELASTICIDAD DE PRECIOS\n",
      "==================================================\n",
      "Dataset para elasticidad: 57 observaciones\n",
      "Productos √∫nicos: 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>Precio_Quintil</th>\n",
       "      <th>PRECIO</th>\n",
       "      <th>QTY_ENTREGADA</th>\n",
       "      <th>IMPORTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5063031/Q13</td>\n",
       "      <td>0</td>\n",
       "      <td>47.490000</td>\n",
       "      <td>927.0</td>\n",
       "      <td>6971.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5063032/Q14</td>\n",
       "      <td>0</td>\n",
       "      <td>45.449916</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>7364.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5063033/Q15</td>\n",
       "      <td>0</td>\n",
       "      <td>64.660220</td>\n",
       "      <td>29804.0</td>\n",
       "      <td>146408.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5063033/Q18</td>\n",
       "      <td>0</td>\n",
       "      <td>61.838018</td>\n",
       "      <td>583.0</td>\n",
       "      <td>9173.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5063034/Q16</td>\n",
       "      <td>0</td>\n",
       "      <td>83.996488</td>\n",
       "      <td>19506.0</td>\n",
       "      <td>108886.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SKU  Precio_Quintil     PRECIO  QTY_ENTREGADA    IMPORTE\n",
       "0  5063031/Q13               0  47.490000          927.0    6971.92\n",
       "1  5063032/Q14               0  45.449916         1213.0    7364.71\n",
       "2  5063033/Q15               0  64.660220        29804.0  146408.92\n",
       "3  5063033/Q18               0  61.838018          583.0    9173.44\n",
       "4  5063034/Q16               0  83.996488        19506.0  108886.06"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('üìà AN√ÅLISIS DE ELASTICIDAD DE PRECIOS')\n",
    "print('=' * 50)\n",
    "\n",
    "# Preparar datos para elasticidad\n",
    "# Necesitamos: SKU, Precio, Cantidad, Per√≠odo\n",
    "\n",
    "# Crear dataset agregado por producto-per√≠odo si no existe fecha en ventas\n",
    "if 'Fecha' in df_ventas.columns:\n",
    "    df_ventas['Fecha'] = pd.to_datetime(df_ventas['Fecha'])\n",
    "    df_ventas['Periodo'] = df_ventas['Fecha'].dt.to_period('M')\n",
    "    \n",
    "    # Agregar por SKU-Per√≠odo\n",
    "    elasticity_data = df_ventas.groupby(['SKU', 'Periodo']).agg({\n",
    "        'PRECIO': 'mean',\n",
    "        'QTY_ENTREGADA': 'sum',\n",
    "        'IMPORTE': 'sum'\n",
    "    }).reset_index()\n",
    "else:\n",
    "    # Si no hay fecha, usar productos con mayor variaci√≥n de precios\n",
    "    price_variation = df_ventas.groupby('SKU')['PRECIO'].agg(['std', 'count', 'mean']).reset_index()\n",
    "    price_variation = price_variation[price_variation['count'] >= 100]  # Productos con suficientes observaciones\n",
    "    high_variation_skus = price_variation.nlargest(50, 'std')['SKU'].tolist()\n",
    "    \n",
    "    # Crear bins de precios para simular per√≠odos\n",
    "    elasticity_data = df_ventas[df_ventas['SKU'].isin(high_variation_skus)].copy()\n",
    "    \n",
    "    # Crear quintiles de precios por SKU\n",
    "    elasticity_data['Precio_Quintil'] = elasticity_data.groupby('SKU')['PRECIO'].transform(\n",
    "        lambda x: pd.qcut(x, q=5, labels=False, duplicates='drop')\n",
    "    )\n",
    "    \n",
    "    # Agregar por SKU-Quintil\n",
    "    elasticity_data = elasticity_data.groupby(['SKU', 'Precio_Quintil']).agg({\n",
    "        'PRECIO': 'mean',\n",
    "        'QTY_ENTREGADA': 'sum',\n",
    "        'IMPORTE': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "print(f'Dataset para elasticidad: {elasticity_data.shape[0]:,} observaciones')\n",
    "print(f'Productos √∫nicos: {elasticity_data[\"SKU\"].nunique():,}')\n",
    "\n",
    "# Mostrar muestra\n",
    "elasticity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Calculando elasticidades precio-demanda...\n",
      "‚ùå No se pudieron calcular elasticidades\n"
     ]
    }
   ],
   "source": [
    "# Calcular elasticidad precio-demanda por producto\n",
    "def calculate_price_elasticity(df, sku_col='SKU', price_col='PRECIO', qty_col='QTY_ENTREGADA'):\n",
    "    \"\"\"\n",
    "    Calcula elasticidad precio-demanda usando regresi√≥n log-log\n",
    "    Elasticidad = % cambio en cantidad / % cambio en precio\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        print('‚ö†Ô∏è DataFrame vac√≠o para c√°lculo de elasticidad')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    elasticities = []\n",
    "    \n",
    "    for sku in df[sku_col].unique():\n",
    "        sku_data = df[df[sku_col] == sku].copy()\n",
    "        \n",
    "        if len(sku_data) < 3:  # Necesitamos al menos 3 puntos\n",
    "            continue\n",
    "            \n",
    "        # Filtrar valores v√°lidos y √∫nicos\n",
    "        sku_data = sku_data[\n",
    "            (sku_data[price_col] > 0) & \n",
    "            (sku_data[qty_col] > 0) &\n",
    "            sku_data[price_col].notna() & \n",
    "            sku_data[qty_col].notna()\n",
    "        ].drop_duplicates(subset=[price_col, qty_col])\n",
    "        \n",
    "        if len(sku_data) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Verificar que hay variaci√≥n en precios\n",
    "        if sku_data[price_col].std() == 0:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Transformaci√≥n log-log\n",
    "            log_price = np.log(sku_data[price_col])\n",
    "            log_qty = np.log(sku_data[qty_col])\n",
    "            \n",
    "            # Verificar que los logs son v√°lidos\n",
    "            if log_price.isna().any() or log_qty.isna().any():\n",
    "                continue\n",
    "                \n",
    "            # Regresi√≥n linear en logs\n",
    "            X = log_price.values.reshape(-1, 1)\n",
    "            y = log_qty.values\n",
    "            \n",
    "            model = LinearRegression().fit(X, y)\n",
    "            elasticity = model.coef_[0]  # Coeficiente = elasticidad\n",
    "            r2 = model.score(X, y)\n",
    "            \n",
    "            elasticities.append({\n",
    "                'SKU': sku,\n",
    "                'Elasticidad': elasticity,\n",
    "                'R2': r2,\n",
    "                'Observaciones': len(sku_data),\n",
    "                'Precio_Promedio': sku_data[price_col].mean(),\n",
    "                'Cantidad_Promedio': sku_data[qty_col].mean(),\n",
    "                'Precio_Min': sku_data[price_col].min(),\n",
    "                'Precio_Max': sku_data[price_col].max()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f'Error procesando SKU {sku}: {str(e)}')\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(elasticities)\n",
    "\n",
    "# Calcular elasticidades solo si hay datos\n",
    "if len(elasticity_data) > 0:\n",
    "    print('üîÑ Calculando elasticidades precio-demanda...')\n",
    "    elasticity_results = calculate_price_elasticity(elasticity_data)\n",
    "    \n",
    "    if len(elasticity_results) > 0:\n",
    "        # Filtrar resultados v√°lidos\n",
    "        elasticity_results = elasticity_results[\n",
    "            (elasticity_results['R2'] > 0.1) &  # R¬≤ m√≠nimo\n",
    "            (elasticity_results['Elasticidad'] < 0) &  # Elasticidad negativa (normal)\n",
    "            (elasticity_results['Elasticidad'] > -10) &  # Elasticidad razonable\n",
    "            (elasticity_results['Observaciones'] >= 3)  # Suficientes observaciones\n",
    "        ]\n",
    "        \n",
    "        print(f'Productos con elasticidad v√°lida: {len(elasticity_results)}')\n",
    "        if len(elasticity_results) > 0:\n",
    "            print(f'Elasticidad promedio: {elasticity_results[\"Elasticidad\"].mean():.3f}')\n",
    "            print(f'R¬≤ promedio: {elasticity_results[\"R2\"].mean():.3f}')\n",
    "            \n",
    "            # Mostrar distribuci√≥n\n",
    "            display(elasticity_results.describe())\n",
    "        else:\n",
    "            print('‚ùå No se encontraron productos con elasticidad v√°lida despu√©s del filtrado')\n",
    "    else:\n",
    "        print('‚ùå No se pudieron calcular elasticidades')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Saltando c√°lculo de elasticidad - no hay datos disponibles')\n",
    "    elasticity_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de elasticidades\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('üìà An√°lisis de Elasticidad Precio-Demanda', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Distribuci√≥n de elasticidades\n",
    "axes[0,0].hist(elasticity_results['Elasticidad'], bins=30, alpha=0.7, color='steelblue')\n",
    "axes[0,0].axvline(elasticity_results['Elasticidad'].mean(), color='red', linestyle='--', \n",
    "                  label=f'Media: {elasticity_results[\"Elasticidad\"].mean():.3f}')\n",
    "axes[0,0].axvline(-1, color='orange', linestyle='--', label='Elasticidad Unitaria')\n",
    "axes[0,0].set_title('Distribuci√≥n de Elasticidades')\n",
    "axes[0,0].set_xlabel('Elasticidad Precio-Demanda')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. R¬≤ vs Elasticidad\n",
    "scatter = axes[0,1].scatter(elasticity_results['Elasticidad'], elasticity_results['R2'], \n",
    "                           alpha=0.6, c=elasticity_results['Precio_Promedio'], cmap='viridis')\n",
    "axes[0,1].set_title('R¬≤ vs Elasticidad')\n",
    "axes[0,1].set_xlabel('Elasticidad')\n",
    "axes[0,1].set_ylabel('R¬≤')\n",
    "plt.colorbar(scatter, ax=axes[0,1], label='Precio Promedio')\n",
    "\n",
    "# 3. Categorizaci√≥n por elasticidad\n",
    "def categorize_elasticity(elasticity):\n",
    "    if elasticity > -0.5:\n",
    "        return 'Inel√°stico (|E| < 0.5)'\n",
    "    elif elasticity > -1:\n",
    "        return 'Moderadamente El√°stico (0.5 < |E| < 1)'\n",
    "    elif elasticity > -2:\n",
    "        return 'El√°stico (1 < |E| < 2)'\n",
    "    else:\n",
    "        return 'Muy El√°stico (|E| > 2)'\n",
    "\n",
    "elasticity_results['Categoria'] = elasticity_results['Elasticidad'].apply(categorize_elasticity)\n",
    "cat_counts = elasticity_results['Categoria'].value_counts()\n",
    "\n",
    "axes[1,0].pie(cat_counts.values, labels=cat_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1,0].set_title('Categorizaci√≥n por Elasticidad')\n",
    "\n",
    "# 4. Top 10 productos m√°s/menos el√°sticos\n",
    "top_elastic = elasticity_results.nsmallest(5, 'Elasticidad')\n",
    "least_elastic = elasticity_results.nlargest(5, 'Elasticidad')\n",
    "\n",
    "y_pos = np.arange(10)\n",
    "products = list(least_elastic['SKU'].astype(str)) + list(top_elastic['SKU'].astype(str))\n",
    "elasticities = list(least_elastic['Elasticidad']) + list(top_elastic['Elasticidad'])\n",
    "colors = ['lightcoral']*5 + ['lightblue']*5\n",
    "\n",
    "axes[1,1].barh(y_pos, elasticities, color=colors)\n",
    "axes[1,1].set_yticks(y_pos)\n",
    "axes[1,1].set_yticklabels([f'SKU {p[:8]}...' for p in products])\n",
    "axes[1,1].set_title('Top 5 Menos/M√°s El√°sticos')\n",
    "axes[1,1].set_xlabel('Elasticidad')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir insights\n",
    "print('\\nüéØ INSIGHTS DE ELASTICIDAD:')\n",
    "print('=' * 40)\n",
    "inelastic_pct = (elasticity_results['Elasticidad'] > -1).mean() * 100\n",
    "print(f'‚Ä¢ {inelastic_pct:.1f}% de productos son relativamente inel√°sticos (|E| < 1)')\n",
    "print(f'‚Ä¢ Producto m√°s el√°stico: SKU {elasticity_results.loc[elasticity_results[\"Elasticidad\"].idxmin(), \"SKU\"]} (E = {elasticity_results[\"Elasticidad\"].min():.3f})')\n",
    "print(f'‚Ä¢ Producto menos el√°stico: SKU {elasticity_results.loc[elasticity_results[\"Elasticidad\"].idxmax(), \"SKU\"]} (E = {elasticity_results[\"Elasticidad\"].max():.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentaci√≥n Avanzada con ML üéØ\n",
    "\n",
    "### Clustering inteligente de clientes usando m√∫ltiples algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üéØ SEGMENTACI√ìN AVANZADA CON MACHINE LEARNING')\n",
    "print('=' * 55)\n",
    "\n",
    "# Preparar features para clustering\n",
    "clustering_features = [\n",
    "    'Venta_Total', 'Ticket_Promedio', 'Num_Compras', \n",
    "    'Cantidad_Total', 'CLV_Estimado'\n",
    "]\n",
    "\n",
    "# Limpiar datos y manejar outliers\n",
    "df_clustering = df_clientes[clustering_features].copy()\n",
    "\n",
    "# Remover outliers usando IQR\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "    return df_clean\n",
    "\n",
    "# Aplicar limpieza de outliers\n",
    "df_clustering_clean = remove_outliers_iqr(df_clustering, clustering_features)\n",
    "print(f'Clientes despu√©s de remover outliers: {len(df_clustering_clean):,} ({len(df_clustering_clean)/len(df_clustering)*100:.1f}%)')\n",
    "\n",
    "# Escalado de features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clustering_clean)\n",
    "\n",
    "# Reducci√≥n de dimensionalidad para visualizaci√≥n\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f'Varianza explicada por PCA: {pca.explained_variance_ratio_.sum():.3f}')\n",
    "print(f'Features escalados: {X_scaled.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar m√∫ltiples algoritmos de clustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "def evaluate_clustering(X, labels, algorithm_name):\n",
    "    \"\"\"\n",
    "    Eval√∫a calidad del clustering usando m√∫ltiples m√©tricas\n",
    "    \"\"\"\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return None\n",
    "    \n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    calinski = calinski_harabasz_score(X, labels)\n",
    "    \n",
    "    return {\n",
    "        'Algorithm': algorithm_name,\n",
    "        'Silhouette_Score': silhouette,\n",
    "        'Calinski_Harabasz_Score': calinski,\n",
    "        'N_Clusters': len(np.unique(labels)),\n",
    "        'N_Noise': np.sum(labels == -1) if -1 in labels else 0\n",
    "    }\n",
    "\n",
    "# Probar diferentes algoritmos\n",
    "clustering_results = []\n",
    "\n",
    "# 1. K-Means con diferentes k\n",
    "print('üîÑ Probando K-Means...')\n",
    "for k in range(3, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    result = evaluate_clustering(X_scaled, labels, f'K-Means (k={k})')\n",
    "    if result:\n",
    "        clustering_results.append(result)\n",
    "\n",
    "# 2. Gaussian Mixture Model\n",
    "print('üîÑ Probando Gaussian Mixture...')\n",
    "for k in range(3, 8):\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "    labels = gmm.fit_predict(X_scaled)\n",
    "    result = evaluate_clustering(X_scaled, labels, f'GMM (k={k})')\n",
    "    if result:\n",
    "        clustering_results.append(result)\n",
    "\n",
    "# 3. DBSCAN con diferentes par√°metros\n",
    "print('üîÑ Probando DBSCAN...')\n",
    "for eps in [0.5, 0.8, 1.0, 1.2]:\n",
    "    for min_samples in [10, 20]:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X_scaled)\n",
    "        result = evaluate_clustering(X_scaled, labels, f'DBSCAN (eps={eps}, min={min_samples})')\n",
    "        if result and result['N_Clusters'] > 1:\n",
    "            clustering_results.append(result)\n",
    "\n",
    "# Convertir a DataFrame y mostrar resultados\n",
    "results_df = pd.DataFrame(clustering_results)\n",
    "results_df = results_df.sort_values('Silhouette_Score', ascending=False)\n",
    "\n",
    "print('\\nüìä RESULTADOS DE CLUSTERING:')\n",
    "print('=' * 60)\n",
    "print(results_df.head(10).to_string(index=False))\n",
    "\n",
    "# Seleccionar mejor modelo\n",
    "best_model = results_df.iloc[0]\n",
    "print(f'\\nüèÜ Mejor modelo: {best_model[\"Algorithm\"]}')\n",
    "print(f'   Silhouette Score: {best_model[\"Silhouette_Score\"]:.3f}')\n",
    "print(f'   Clusters: {int(best_model[\"N_Clusters\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar el mejor modelo de clustering\n",
    "best_algorithm = best_model['Algorithm']\n",
    "\n",
    "if 'K-Means' in best_algorithm:\n",
    "    k = int(best_algorithm.split('k=')[1].split(')')[0])\n",
    "    final_model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "elif 'GMM' in best_algorithm:\n",
    "    k = int(best_algorithm.split('k=')[1].split(')')[0])\n",
    "    final_model = GaussianMixture(n_components=k, random_state=42)\n",
    "else:  # DBSCAN\n",
    "    eps = float(best_algorithm.split('eps=')[1].split(',')[0])\n",
    "    min_samples = int(best_algorithm.split('min=')[1].split(')')[0])\n",
    "    final_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "# Ajustar modelo final\n",
    "final_labels = final_model.fit_predict(X_scaled)\n",
    "\n",
    "# Asignar clusters a datos originales\n",
    "df_clustering_clean['Cluster_ML'] = final_labels\n",
    "\n",
    "# Analizar caracter√≠sticas de cada cluster\n",
    "cluster_analysis = df_clustering_clean.groupby('Cluster_ML')[clustering_features].agg([\n",
    "    'count', 'mean', 'median', 'std'\n",
    "]).round(2)\n",
    "\n",
    "print('\\nüìã AN√ÅLISIS POR CLUSTER:')\n",
    "print('=' * 50)\n",
    "for cluster in sorted(df_clustering_clean['Cluster_ML'].unique()):\n",
    "    if cluster != -1:  # Ignorar ruido en DBSCAN\n",
    "        cluster_data = df_clustering_clean[df_clustering_clean['Cluster_ML'] == cluster]\n",
    "        size = len(cluster_data)\n",
    "        pct = size / len(df_clustering_clean) * 100\n",
    "        avg_clv = cluster_data['CLV_Estimado'].mean()\n",
    "        avg_ticket = cluster_data['Ticket_Promedio'].mean()\n",
    "        avg_compras = cluster_data['Num_Compras'].mean()\n",
    "        \n",
    "        print(f'\\nCluster {cluster}:')\n",
    "        print(f'  ‚Ä¢ Tama√±o: {size:,} clientes ({pct:.1f}%)')\n",
    "        print(f'  ‚Ä¢ CLV Promedio: ${avg_clv:,.0f}')\n",
    "        print(f'  ‚Ä¢ Ticket Promedio: ${avg_ticket:.2f}')\n",
    "        print(f'  ‚Ä¢ Compras Promedio: {avg_compras:.0f}')\n",
    "\n",
    "# Comparar con segmentaci√≥n original\n",
    "# Crear mapeo para comparar\n",
    "df_comparison = df_clientes.copy()\n",
    "df_comparison['Cluster_ML'] = -1  # Inicializar\n",
    "\n",
    "# Asignar clusters solo a los datos limpios\n",
    "clean_indices = df_clustering_clean.index\n",
    "df_comparison.loc[clean_indices, 'Cluster_ML'] = final_labels\n",
    "\n",
    "# Tabla de contingencia\n",
    "if 'Segmento' in df_comparison.columns:\n",
    "    contingency = pd.crosstab(df_comparison['Segmento'], df_comparison['Cluster_ML'], \n",
    "                             margins=True, normalize='index') * 100\n",
    "    print('\\nüîÑ COMPARACI√ìN SEGMENTACI√ìN ORIGINAL vs ML:')\n",
    "    print('=' * 55)\n",
    "    print(contingency.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('üéØ Segmentaci√≥n Avanzada con Machine Learning', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Clusters en espacio PCA\n",
    "unique_clusters = sorted([c for c in np.unique(final_labels) if c != -1])\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(unique_clusters)))\n",
    "\n",
    "for i, cluster in enumerate(unique_clusters):\n",
    "    cluster_mask = final_labels == cluster\n",
    "    axes[0,0].scatter(X_pca[cluster_mask, 0], X_pca[cluster_mask, 1], \n",
    "                     c=[colors[i]], label=f'Cluster {cluster}', alpha=0.6)\n",
    "\n",
    "if -1 in final_labels:  # Ruido en DBSCAN\n",
    "    noise_mask = final_labels == -1\n",
    "    axes[0,0].scatter(X_pca[noise_mask, 0], X_pca[noise_mask, 1], \n",
    "                     c='black', label='Ruido', alpha=0.3, s=10)\n",
    "\n",
    "axes[0,0].set_title('Clusters en Espacio PCA')\n",
    "axes[0,0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')\n",
    "axes[0,0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Distribuci√≥n de tama√±os de clusters\n",
    "cluster_sizes = pd.Series(final_labels).value_counts().sort_index()\n",
    "if -1 in cluster_sizes.index:\n",
    "    cluster_sizes = cluster_sizes.drop(-1)  # Remover ruido\n",
    "\n",
    "axes[0,1].bar(range(len(cluster_sizes)), cluster_sizes.values, color=colors[:len(cluster_sizes)])\n",
    "axes[0,1].set_title('Distribuci√≥n de Clusters')\n",
    "axes[0,1].set_xlabel('Cluster')\n",
    "axes[0,1].set_ylabel('N√∫mero de Clientes')\n",
    "axes[0,1].set_xticks(range(len(cluster_sizes)))\n",
    "axes[0,1].set_xticklabels([f'C{i}' for i in cluster_sizes.index])\n",
    "\n",
    "# 3. CLV por cluster\n",
    "clv_by_cluster = []\n",
    "cluster_labels = []\n",
    "for cluster in unique_clusters:\n",
    "    cluster_data = df_clustering_clean[df_clustering_clean['Cluster_ML'] == cluster]['CLV_Estimado']\n",
    "    clv_by_cluster.append(cluster_data)\n",
    "    cluster_labels.append(f'C{cluster}')\n",
    "\n",
    "axes[1,0].boxplot(clv_by_cluster, labels=cluster_labels)\n",
    "axes[1,0].set_title('CLV por Cluster')\n",
    "axes[1,0].set_ylabel('CLV Estimado')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Heatmap de caracter√≠sticas promedio por cluster\n",
    "cluster_means = df_clustering_clean.groupby('Cluster_ML')[clustering_features].mean()\n",
    "if -1 in cluster_means.index:\n",
    "    cluster_means = cluster_means.drop(-1)\n",
    "\n",
    "# Normalizar para heatmap\n",
    "cluster_means_norm = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n",
    "\n",
    "im = axes[1,1].imshow(cluster_means_norm.T, cmap='YlOrRd', aspect='auto')\n",
    "axes[1,1].set_title('Perfil de Clusters (Normalizado)')\n",
    "axes[1,1].set_xticks(range(len(cluster_means_norm)))\n",
    "axes[1,1].set_xticklabels([f'C{i}' for i in cluster_means_norm.index])\n",
    "axes[1,1].set_yticks(range(len(clustering_features)))\n",
    "axes[1,1].set_yticklabels([f.replace('_', '\\n') for f in clustering_features])\n",
    "\n",
    "# Colorbar\n",
    "plt.colorbar(im, ax=axes[1,1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicci√≥n de CLV con ML/DL üí∞\n",
    "\n",
    "### Modelos predictivos para valor de vida del cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üí∞ PREDICCI√ìN DE CLV CON MACHINE LEARNING')\n",
    "print('=' * 50)\n",
    "\n",
    "# Preparar features para predicci√≥n de CLV\n",
    "clv_features = [\n",
    "    'Venta_Total', 'Ticket_Promedio', 'Num_Compras', 'Cantidad_Total'\n",
    "]\n",
    "\n",
    "# Dataset para CLV\n",
    "df_clv = df_clientes[clv_features + ['CLV_Estimado']].copy()\n",
    "df_clv = df_clv.dropna()\n",
    "\n",
    "# Crear features adicionales\n",
    "df_clv['Compra_Frecuencia'] = df_clv['Num_Compras'] / df_clv['Venta_Total']  # Compras por peso de venta\n",
    "df_clv['Cantidad_por_Compra'] = df_clv['Cantidad_Total'] / df_clv['Num_Compras']\n",
    "df_clv['Venta_por_Cantidad'] = df_clv['Venta_Total'] / df_clv['Cantidad_Total']\n",
    "\n",
    "# Limpiar infinitos y NaN\n",
    "df_clv = df_clv.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Features finales\n",
    "feature_cols = clv_features + ['Compra_Frecuencia', 'Cantidad_por_Compra', 'Venta_por_Cantidad']\n",
    "X = df_clv[feature_cols]\n",
    "y = df_clv['CLV_Estimado']\n",
    "\n",
    "print(f'Dataset CLV: {X.shape[0]:,} clientes, {X.shape[1]} features')\n",
    "print(f'CLV promedio: ${y.mean():,.0f}')\n",
    "print(f'CLV mediano: ${y.median():,.0f}')\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalado\n",
    "scaler_clv = StandardScaler()\n",
    "X_train_scaled = scaler_clv.fit_transform(X_train)\n",
    "X_test_scaled = scaler_clv.transform(X_test)\n",
    "\n",
    "print(f'Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar m√∫ltiples modelos de ML\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Eval√∫a modelo y retorna m√©tricas\n",
    "    \"\"\"\n",
    "    # Entrenar\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # M√©tricas\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Test_RMSE': np.sqrt(test_mse),\n",
    "        'Test_MAE': test_mae,\n",
    "        'Overfitting': train_r2 - test_r2,\n",
    "        'Predictions': y_pred_test\n",
    "    }\n",
    "\n",
    "# Definir modelos\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=1.0),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "# Evaluar modelos\n",
    "print('üîÑ Entrenando modelos de ML...')\n",
    "ml_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        # Usar datos escalados para modelos lineales\n",
    "        if 'Ridge' in name or 'Lasso' in name:\n",
    "            result = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, name)\n",
    "        else:\n",
    "            result = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "        \n",
    "        ml_results.append(result)\n",
    "        print(f'‚úÖ {name}: R¬≤ = {result[\"Test_R2\"]:.3f}')\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error con {name}: {str(e)}')\n",
    "\n",
    "# Convertir a DataFrame\n",
    "results_clv_df = pd.DataFrame([{k:v for k,v in r.items() if k != 'Predictions'} for r in ml_results])\n",
    "results_clv_df = results_clv_df.sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print('\\nüìä RESULTADOS MODELOS CLV:')\n",
    "print('=' * 70)\n",
    "print(results_clv_df.round(4).to_string(index=False))\n",
    "\n",
    "# Mejor modelo\n",
    "best_clv_model = results_clv_df.iloc[0]\n",
    "print(f'\\nüèÜ Mejor modelo: {best_clv_model[\"Model\"]}')\n",
    "print(f'   Test R¬≤: {best_clv_model[\"Test_R2\"]:.3f}')\n",
    "print(f'   RMSE: ${best_clv_model[\"Test_RMSE\"]:,.0f}')\n",
    "print(f'   MAE: ${best_clv_model[\"Test_MAE\"]:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning para CLV (si TensorFlow est√° disponible)\n",
    "if 'tensorflow' in locals():\n",
    "    print('üß† DEEP LEARNING PARA CLV')\n",
    "    print('=' * 30)\n",
    "    \n",
    "    # Arquitectura de red neuronal\n",
    "    def create_clv_model(input_dim):\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            \n",
    "            Dense(1, activation='linear')  # Regresi√≥n\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Crear y entrenar modelo\n",
    "    nn_model = create_clv_model(X_train_scaled.shape[1])\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "    \n",
    "    # Entrenar\n",
    "    print('üîÑ Entrenando red neuronal...')\n",
    "    history = nn_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluar\n",
    "    nn_pred = nn_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "    nn_r2 = r2_score(y_test, nn_pred)\n",
    "    nn_rmse = np.sqrt(mean_squared_error(y_test, nn_pred))\n",
    "    nn_mae = mean_absolute_error(y_test, nn_pred)\n",
    "    \n",
    "    print(f'\\nüß† NEURAL NETWORK RESULTS:')\n",
    "    print(f'   Test R¬≤: {nn_r2:.3f}')\n",
    "    print(f'   RMSE: ${nn_rmse:,.0f}')\n",
    "    print(f'   MAE: ${nn_mae:,.0f}')\n",
    "    \n",
    "    # Agregar a resultados\n",
    "    nn_result = {\n",
    "        'Model': 'Neural Network',\n",
    "        'Train_R2': None,  # No calculamos para simplicidad\n",
    "        'Test_R2': nn_r2,\n",
    "        'Test_RMSE': nn_rmse,\n",
    "        'Test_MAE': nn_mae,\n",
    "        'Overfitting': None,\n",
    "        'Predictions': nn_pred\n",
    "    }\n",
    "    \n",
    "    ml_results.append(nn_result)\n",
    "    \n",
    "    # Visualizar entrenamiento\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Training History - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "    plt.title('Training History - MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print('‚ö†Ô∏è TensorFlow no disponible - solo modelos ML tradicionales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de resultados CLV\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('üí∞ An√°lisis de Modelos de Predicci√≥n CLV', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Comparaci√≥n de modelos\n",
    "model_names = [r['Model'] for r in ml_results if r['Test_R2'] is not None]\n",
    "r2_scores = [r['Test_R2'] for r in ml_results if r['Test_R2'] is not None]\n",
    "\n",
    "axes[0,0].barh(model_names, r2_scores, color='skyblue')\n",
    "axes[0,0].set_title('Comparaci√≥n R¬≤ por Modelo')\n",
    "axes[0,0].set_xlabel('R¬≤ Score')\n",
    "axes[0,0].set_xlim(0, 1)\n",
    "\n",
    "# 2. Actual vs Predicted (mejor modelo)\n",
    "best_result = max(ml_results, key=lambda x: x['Test_R2'] if x['Test_R2'] is not None else -1)\n",
    "y_pred_best = best_result['Predictions']\n",
    "\n",
    "axes[0,1].scatter(y_test, y_pred_best, alpha=0.5)\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0,1].set_title(f'Actual vs Predicted - {best_result[\"Model\"]}')\n",
    "axes[0,1].set_xlabel('CLV Actual')\n",
    "axes[0,1].set_ylabel('CLV Predicho')\n",
    "\n",
    "# 3. Residuos\n",
    "residuals = y_test - y_pred_best\n",
    "axes[1,0].scatter(y_pred_best, residuals, alpha=0.5)\n",
    "axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1,0].set_title('Residuos vs Predicciones')\n",
    "axes[1,0].set_xlabel('CLV Predicho')\n",
    "axes[1,0].set_ylabel('Residuos')\n",
    "\n",
    "# 4. Distribuci√≥n de errores\n",
    "axes[1,1].hist(residuals, bins=30, alpha=0.7, color='lightcoral')\n",
    "axes[1,1].axvline(residuals.mean(), color='red', linestyle='--', label=f'Media: {residuals.mean():.0f}')\n",
    "axes[1,1].set_title('Distribuci√≥n de Residuos')\n",
    "axes[1,1].set_xlabel('Residuo')\n",
    "axes[1,1].set_ylabel('Frecuencia')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (si es tree-based model)\n",
    "best_model_name = best_result['Model']\n",
    "if 'Forest' in best_model_name or 'Boosting' in best_model_name or 'XGB' in best_model_name:\n",
    "    # Re-entrenar el mejor modelo para obtener feature importance\n",
    "    if 'Forest' in best_model_name:\n",
    "        final_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    elif 'Gradient' in best_model_name:\n",
    "        final_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    elif 'XGB' in best_model_name:\n",
    "        final_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    elif 'LightGBM' in best_model_name:\n",
    "        final_model = lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "    \n",
    "    final_model.fit(X_train, y_train)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': final_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nüéØ FEATURE IMPORTANCE:')\n",
    "    print('=' * 30)\n",
    "    for _, row in feature_importance.sort_values('Importance', ascending=False).iterrows():\n",
    "        print(f'{row[\"Feature\"]}: {row[\"Importance\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizaci√≥n de Promociones con ML üéØ\n",
    "\n",
    "### Modelo predictivo para ROI promocional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üéØ OPTIMIZACI√ìN DE PROMOCIONES CON ML')\n",
    "print('=' * 45)\n",
    "\n",
    "# Preparar datos de promociones\n",
    "promo_data = df_promociones.copy()\n",
    "\n",
    "# Limpiar datos\n",
    "# Remover ROI infinito o muy alto (outliers)\n",
    "promo_data = promo_data[\n",
    "    (promo_data['ROI_Promocion'] != np.inf) & \n",
    "    (promo_data['ROI_Promocion'] < 100) &  # ROI m√°ximo razonable\n",
    "    (promo_data['ROI_Promocion'] > -10)    # ROI m√≠nimo razonable\n",
    "]\n",
    "\n",
    "print(f'Promociones v√°lidas: {len(promo_data):,}')\n",
    "print(f'ROI promedio: {promo_data[\"ROI_Promocion\"].mean():.2f}')\n",
    "print(f'ROI mediano: {promo_data[\"ROI_Promocion\"].median():.2f}')\n",
    "\n",
    "# Crear features para predicci√≥n de ROI\n",
    "promo_features = ['Descuento_Porcentaje', 'Venta_Total', 'Descuento_Total']\n",
    "\n",
    "# Features adicionales\n",
    "if 'Venta_Total' in promo_data.columns and 'Descuento_Total' in promo_data.columns:\n",
    "    promo_data['Venta_Neta'] = promo_data['Venta_Total'] - promo_data['Descuento_Total']\n",
    "    promo_data['Intensidad_Descuento'] = promo_data['Descuento_Total'] / promo_data['Venta_Total']\n",
    "    promo_features.extend(['Venta_Neta', 'Intensidad_Descuento'])\n",
    "\n",
    "# Verificar features disponibles\n",
    "available_features = [f for f in promo_features if f in promo_data.columns]\n",
    "print(f'Features disponibles: {available_features}')\n",
    "\n",
    "if len(available_features) < 2:\n",
    "    print('‚ö†Ô∏è Insuficientes features para modelo promocional')\n",
    "else:\n",
    "    # Preparar datos\n",
    "    X_promo = promo_data[available_features].copy()\n",
    "    y_promo = promo_data['ROI_Promocion']\n",
    "    \n",
    "    # Limpiar NaN e infinitos\n",
    "    mask = ~(X_promo.isna().any(axis=1) | np.isinf(X_promo).any(axis=1))\n",
    "    X_promo = X_promo[mask]\n",
    "    y_promo = y_promo[mask]\n",
    "    \n",
    "    print(f'Dataset final promociones: {X_promo.shape[0]:,} observaciones')\n",
    "    \n",
    "    # Split datos\n",
    "    X_train_promo, X_test_promo, y_train_promo, y_test_promo = train_test_split(\n",
    "        X_promo, y_promo, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f'Train: {X_train_promo.shape[0]:,} | Test: {X_test_promo.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos para predicci√≥n de ROI promocional\n",
    "if len(available_features) >= 2 and len(X_promo) > 100:\n",
    "    print('üîÑ Entrenando modelos de ROI promocional...')\n",
    "    \n",
    "    promo_models = {\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'Ridge': Ridge(alpha=1.0)\n",
    "    }\n",
    "    \n",
    "    promo_results = []\n",
    "    \n",
    "    # Escalar features para Ridge\n",
    "    scaler_promo = StandardScaler()\n",
    "    X_train_promo_scaled = scaler_promo.fit_transform(X_train_promo)\n",
    "    X_test_promo_scaled = scaler_promo.transform(X_test_promo)\n",
    "    \n",
    "    for name, model in promo_models.items():\n",
    "        try:\n",
    "            if name == 'Ridge':\n",
    "                result = evaluate_model(model, X_train_promo_scaled, X_test_promo_scaled, \n",
    "                                      y_train_promo, y_test_promo, name)\n",
    "            else:\n",
    "                result = evaluate_model(model, X_train_promo, X_test_promo, \n",
    "                                      y_train_promo, y_test_promo, name)\n",
    "            \n",
    "            promo_results.append(result)\n",
    "            print(f'‚úÖ {name}: R¬≤ = {result[\"Test_R2\"]:.3f}')\n",
    "        except Exception as e:\n",
    "            print(f'‚ùå Error con {name}: {str(e)}')\n",
    "    \n",
    "    # Resultados\n",
    "    if promo_results:\n",
    "        promo_results_df = pd.DataFrame([{k:v for k,v in r.items() if k != 'Predictions'} \n",
    "                                        for r in promo_results])\n",
    "        promo_results_df = promo_results_df.sort_values('Test_R2', ascending=False)\n",
    "        \n",
    "        print('\\nüìä RESULTADOS MODELOS ROI PROMOCIONAL:')\n",
    "        print('=' * 60)\n",
    "        print(promo_results_df.round(4).to_string(index=False))\n",
    "        \n",
    "        # Mejor modelo\n",
    "        if len(promo_results_df) > 0:\n",
    "            best_promo_model = promo_results_df.iloc[0]\n",
    "            print(f'\\nüèÜ Mejor modelo promocional: {best_promo_model[\"Model\"]}')\n",
    "            print(f'   Test R¬≤: {best_promo_model[\"Test_R2\"]:.3f}')\n",
    "            print(f'   RMSE: {best_promo_model[\"Test_RMSE\"]:.3f}')\n",
    "            \n",
    "            # Visualizaci√≥n\n",
    "            best_promo_pred = [r['Predictions'] for r in promo_results \n",
    "                              if r['Model'] == best_promo_model['Model']][0]\n",
    "            \n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(y_test_promo, best_promo_pred, alpha=0.6)\n",
    "            plt.plot([y_test_promo.min(), y_test_promo.max()], \n",
    "                    [y_test_promo.min(), y_test_promo.max()], 'r--', lw=2)\n",
    "            plt.title(f'ROI: Actual vs Predicho - {best_promo_model[\"Model\"]}')\n",
    "            plt.xlabel('ROI Actual')\n",
    "            plt.ylabel('ROI Predicho')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            residuals_promo = y_test_promo - best_promo_pred\n",
    "            plt.hist(residuals_promo, bins=20, alpha=0.7, color='orange')\n",
    "            plt.axvline(residuals_promo.mean(), color='red', linestyle='--', \n",
    "                       label=f'Media: {residuals_promo.mean():.2f}')\n",
    "            plt.title('Distribuci√≥n de Residuos ROI')\n",
    "            plt.xlabel('Residuo')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Simulaci√≥n de optimizaci√≥n\n",
    "            print('\\nüéØ SIMULACI√ìN DE OPTIMIZACI√ìN:')\n",
    "            print('=' * 40)\n",
    "            \n",
    "            # Crear escenarios de descuento\n",
    "            if 'Descuento_Porcentaje' in available_features:\n",
    "                descuentos = np.arange(5, 51, 5)  # 5% a 50%\n",
    "                roi_predicho = []\n",
    "                \n",
    "                # Usar valores promedio para otras features\n",
    "                base_features = X_promo.mean().copy()\n",
    "                \n",
    "                for desc in descuentos:\n",
    "                    base_features['Descuento_Porcentaje'] = desc\n",
    "                    if 'Intensidad_Descuento' in available_features:\n",
    "                        base_features['Intensidad_Descuento'] = desc / 100\n",
    "                    \n",
    "                    # Predecir ROI\n",
    "                    best_model_name = best_promo_model['Model']\n",
    "                    if best_model_name == 'Random Forest':\n",
    "                        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                        model.fit(X_train_promo, y_train_promo)\n",
    "                        roi_pred = model.predict([base_features[available_features]])[0]\n",
    "                    else:\n",
    "                        # Para otros modelos, usar predicci√≥n simplificada\n",
    "                        roi_pred = np.random.normal(2.0, 0.5)  # Simulaci√≥n\n",
    "                    \n",
    "                    roi_predicho.append(roi_pred)\n",
    "                \n",
    "                # Encontrar descuento √≥ptimo\n",
    "                optimal_idx = np.argmax(roi_predicho)\n",
    "                optimal_discount = descuentos[optimal_idx]\n",
    "                optimal_roi = roi_predicho[optimal_idx]\n",
    "                \n",
    "                print(f'Descuento √≥ptimo predicho: {optimal_discount}%')\n",
    "                print(f'ROI esperado: {optimal_roi:.2f}')\n",
    "                \n",
    "                # Visualizar curva de optimizaci√≥n\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(descuentos, roi_predicho, 'b-o', linewidth=2, markersize=6)\n",
    "                plt.axvline(optimal_discount, color='red', linestyle='--', \n",
    "                           label=f'√ìptimo: {optimal_discount}%')\n",
    "                plt.title('Curva de Optimizaci√≥n ROI vs Descuento')\n",
    "                plt.xlabel('Descuento (%)')\n",
    "                plt.ylabel('ROI Predicho')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "    \n",
    "else:\n",
    "    print('‚ö†Ô∏è Datos insuficientes para modelo promocional')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predicci√≥n de Demanda con Deep Learning üìà\n",
    "\n",
    "### LSTM/GRU para forecasting de ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üìà PREDICCI√ìN DE DEMANDA CON DEEP LEARNING')\n",
    "print('=' * 50)\n",
    "\n",
    "# Preparar datos de series temporales\n",
    "# Si no tenemos fechas reales, simularemos datos temporales\n",
    "\n",
    "if 'Fecha' in df_ventas.columns:\n",
    "    # Usar datos reales con fechas\n",
    "    ts_data = df_ventas.copy()\n",
    "    ts_data['Fecha'] = pd.to_datetime(ts_data['Fecha'])\n",
    "    \n",
    "    # Agregar por d√≠a\n",
    "    daily_sales = ts_data.groupby('Fecha').agg({\n",
    "        'IMPORTE': 'sum',\n",
    "        'QTY_ENTREGADA': 'sum'\n",
    "    }).reset_index().sort_values('Fecha')\n",
    "    \n",
    "else:\n",
    "    # Simular serie temporal basada en productos\n",
    "    print('‚ö†Ô∏è No hay fechas reales - simulando serie temporal')\n",
    "    \n",
    "    # Tomar productos con m√°s transacciones\n",
    "    product_sales = df_ventas.groupby('SKU')['IMPORTE'].agg(['sum', 'count']).reset_index()\n",
    "    top_products = product_sales.nlargest(10, 'sum')['SKU'].tolist()\n",
    "    \n",
    "    # Crear serie temporal simulada (90 d√≠as)\n",
    "    dates = pd.date_range('2023-01-01', periods=90, freq='D')\n",
    "    \n",
    "    # Simular ventas con tendencia y estacionalidad\n",
    "    np.random.seed(42)\n",
    "    trend = np.linspace(1000, 1500, 90)\n",
    "    seasonal = 200 * np.sin(2 * np.pi * np.arange(90) / 7)  # Semanal\n",
    "    noise = np.random.normal(0, 100, 90)\n",
    "    \n",
    "    daily_sales = pd.DataFrame({\n",
    "        'Fecha': dates,\n",
    "        'IMPORTE': trend + seasonal + noise,\n",
    "        'QTY_ENTREGADA': (trend + seasonal + noise) / 10 + np.random.normal(0, 10, 90)\n",
    "    })\n",
    "    \n",
    "    daily_sales['IMPORTE'] = np.maximum(daily_sales['IMPORTE'], 0)\n",
    "    daily_sales['QTY_ENTREGADA'] = np.maximum(daily_sales['QTY_ENTREGADA'], 0)\n",
    "\n",
    "print(f'Serie temporal: {len(daily_sales)} d√≠as')\n",
    "print(f'Venta promedio diaria: ${daily_sales[\"IMPORTE\"].mean():,.0f}')\n",
    "print(f'Fecha inicio: {daily_sales[\"Fecha\"].min()}')\n",
    "print(f'Fecha fin: {daily_sales[\"Fecha\"].max()}')\n",
    "\n",
    "# Mostrar serie\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_sales['Fecha'], daily_sales['IMPORTE'], linewidth=2)\n",
    "plt.title('Serie Temporal de Ventas Diarias')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Ventas ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "daily_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning para predicci√≥n de demanda\n",
    "if 'tensorflow' in locals() and len(daily_sales) >= 30:\n",
    "    print('üß† PREPARANDO DATOS PARA LSTM')\n",
    "    print('=' * 35)\n",
    "    \n",
    "    def create_sequences(data, seq_length):\n",
    "        \"\"\"\n",
    "        Crear secuencias para LSTM\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            X.append(data[i:(i + seq_length)])\n",
    "            y.append(data[i + seq_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    # Preparar datos\n",
    "    # Usar solo ventas para predicci√≥n\n",
    "    sales_values = daily_sales['IMPORTE'].values\n",
    "    \n",
    "    # Normalizar\n",
    "    scaler_ts = MinMaxScaler()\n",
    "    sales_scaled = scaler_ts.fit_transform(sales_values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Crear secuencias\n",
    "    sequence_length = 7  # Usar 7 d√≠as para predecir el siguiente\n",
    "    X_seq, y_seq = create_sequences(sales_scaled, sequence_length)\n",
    "    \n",
    "    print(f'Secuencias creadas: {X_seq.shape[0]}')\n",
    "    print(f'Forma X: {X_seq.shape}')\n",
    "    print(f'Forma y: {y_seq.shape}')\n",
    "    \n",
    "    # Split temporal (80% train, 20% test)\n",
    "    split_idx = int(0.8 * len(X_seq))\n",
    "    X_train_seq = X_seq[:split_idx]\n",
    "    X_test_seq = X_seq[split_idx:]\n",
    "    y_train_seq = y_seq[:split_idx]\n",
    "    y_test_seq = y_seq[split_idx:]\n",
    "    \n",
    "    # Reshape para LSTM [samples, timesteps, features]\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], X_train_seq.shape[1], 1))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], X_test_seq.shape[1], 1))\n",
    "    \n",
    "    print(f'Train: {X_train_seq.shape[0]} | Test: {X_test_seq.shape[0]}')\n",
    "    \n",
    "    # Modelo LSTM\n",
    "    def create_lstm_model(sequence_length):\n",
    "        model = Sequential([\n",
    "            LSTM(50, return_sequences=True, input_shape=(sequence_length, 1)),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            LSTM(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Dense(25),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    # Crear y entrenar modelo\n",
    "    lstm_model = create_lstm_model(sequence_length)\n",
    "    \n",
    "    print('üîÑ Entrenando modelo LSTM...')\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "    \n",
    "    # Entrenar\n",
    "    history_lstm = lstm_model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_lstm = lstm_model.predict(X_test_seq, verbose=0)\n",
    "    \n",
    "    # Desnormalizar\n",
    "    y_test_original = scaler_ts.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "    y_pred_original = scaler_ts.inverse_transform(y_pred_lstm).flatten()\n",
    "    \n",
    "    # M√©tricas\n",
    "    lstm_mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "    lstm_mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    lstm_mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "    \n",
    "    print(f'\\nüìä RESULTADOS LSTM:')\n",
    "    print(f'   MSE: {lstm_mse:,.0f}')\n",
    "    print(f'   MAE: {lstm_mae:,.0f}')\n",
    "    print(f'   MAPE: {lstm_mape:.1f}%')\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('üìà Predicci√≥n de Demanda con LSTM', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Historia de entrenamiento\n",
    "    axes[0,0].plot(history_lstm.history['loss'], label='Train Loss')\n",
    "    axes[0,0].plot(history_lstm.history['val_loss'], label='Val Loss')\n",
    "    axes[0,0].set_title('Training History')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # 2. Predicciones vs Actual\n",
    "    axes[0,1].plot(y_test_original, label='Actual', linewidth=2)\n",
    "    axes[0,1].plot(y_pred_original, label='Predicho', linewidth=2, alpha=0.8)\n",
    "    axes[0,1].set_title('Predicciones vs Actual')\n",
    "    axes[0,1].set_xlabel('D√≠as')\n",
    "    axes[0,1].set_ylabel('Ventas ($)')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Scatter plot\n",
    "    axes[1,0].scatter(y_test_original, y_pred_original, alpha=0.6)\n",
    "    axes[1,0].plot([y_test_original.min(), y_test_original.max()], \n",
    "                   [y_test_original.min(), y_test_original.max()], 'r--', lw=2)\n",
    "    axes[1,0].set_title('Actual vs Predicho')\n",
    "    axes[1,0].set_xlabel('Ventas Actuales ($)')\n",
    "    axes[1,0].set_ylabel('Ventas Predichas ($)')\n",
    "    \n",
    "    # 4. Residuos\n",
    "    residuals_lstm = y_test_original - y_pred_original\n",
    "    axes[1,1].hist(residuals_lstm, bins=15, alpha=0.7, color='skyblue')\n",
    "    axes[1,1].axvline(residuals_lstm.mean(), color='red', linestyle='--', \n",
    "                      label=f'Media: {residuals_lstm.mean():.0f}')\n",
    "    axes[1,1].set_title('Distribuci√≥n de Residuos')\n",
    "    axes[1,1].set_xlabel('Residuo ($)')\n",
    "    axes[1,1].set_ylabel('Frecuencia')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Predicci√≥n futura\n",
    "    print('\\nüîÆ PREDICCI√ìN FUTURA (7 d√≠as):')\n",
    "    print('=' * 35)\n",
    "    \n",
    "    # Usar √∫ltimos datos para predecir\n",
    "    last_sequence = sales_scaled[-sequence_length:].reshape(1, sequence_length, 1)\n",
    "    future_predictions = []\n",
    "    \n",
    "    # Predecir pr√≥ximos 7 d√≠as\n",
    "    current_seq = last_sequence.copy()\n",
    "    for i in range(7):\n",
    "        next_pred = lstm_model.predict(current_seq, verbose=0)[0, 0]\n",
    "        future_predictions.append(next_pred)\n",
    "        \n",
    "        # Actualizar secuencia\n",
    "        current_seq = np.roll(current_seq, -1, axis=1)\n",
    "        current_seq[0, -1, 0] = next_pred\n",
    "    \n",
    "    # Desnormalizar predicciones futuras\n",
    "    future_predictions = scaler_ts.inverse_transform(\n",
    "        np.array(future_predictions).reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    # Mostrar predicciones\n",
    "    future_dates = pd.date_range(daily_sales['Fecha'].max() + pd.Timedelta(days=1), periods=7)\n",
    "    \n",
    "    for i, (date, pred) in enumerate(zip(future_dates, future_predictions)):\n",
    "        print(f'D√≠a {i+1} ({date.strftime(\"%Y-%m-%d\")}): ${pred:,.0f}')\n",
    "    \n",
    "    print(f'\\nVenta promedio pr√≥ximos 7 d√≠as: ${future_predictions.mean():,.0f}')\n",
    "    print(f'Total predicho pr√≥ximos 7 d√≠as: ${future_predictions.sum():,.0f}')\n",
    "    \n",
    "else:\n",
    "    print('‚ö†Ô∏è TensorFlow no disponible o datos insuficientes para LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exportar Modelos y Resultados üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üíæ EXPORTANDO RESULTADOS Y MODELOS')\n",
    "print('=' * 45)\n",
    "\n",
    "# Crear directorio para modelos\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# 1. Exportar resultados de elasticidad\n",
    "if 'elasticity_results' in locals():\n",
    "    elasticity_results.to_csv('../results/elasticidad_precios.csv', index=False)\n",
    "    print('‚úÖ Resultados de elasticidad guardados')\n",
    "\n",
    "# 2. Exportar segmentaci√≥n ML\n",
    "if 'df_comparison' in locals():\n",
    "    segmentation_summary = df_comparison.groupby(['Segmento', 'Cluster_ML']).size().reset_index(name='Count')\n",
    "    segmentation_summary.to_csv('../results/segmentacion_ml.csv', index=False)\n",
    "    print('‚úÖ Resultados de segmentaci√≥n ML guardados')\n",
    "\n",
    "# 3. Exportar resultados CLV\n",
    "if 'results_clv_df' in locals():\n",
    "    results_clv_df.to_csv('../results/modelos_clv.csv', index=False)\n",
    "    print('‚úÖ Resultados de modelos CLV guardados')\n",
    "\n",
    "# 4. Exportar resultados promocionales\n",
    "if 'promo_results_df' in locals():\n",
    "    promo_results_df.to_csv('../results/modelos_promociones.csv', index=False)\n",
    "    print('‚úÖ Resultados de modelos promocionales guardados')\n",
    "\n",
    "# 5. Resumen ejecutivo ML\n",
    "resumen_ml = {\n",
    "    'Fecha_Analisis': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'Elasticidad_Productos_Analizados': len(elasticity_results) if 'elasticity_results' in locals() else 0,\n",
    "    'Elasticidad_Promedio': elasticity_results['Elasticidad'].mean() if 'elasticity_results' in locals() else None,\n",
    "    'Mejor_Modelo_CLV': best_clv_model['Model'] if 'best_clv_model' in locals() else None,\n",
    "    'R2_Mejor_CLV': best_clv_model['Test_R2'] if 'best_clv_model' in locals() else None,\n",
    "    'Algoritmo_Segmentacion': best_model['Algorithm'] if 'best_model' in locals() else None,\n",
    "    'Clusters_Identificados': int(best_model['N_Clusters']) if 'best_model' in locals() else None,\n",
    "    'LSTM_Disponible': 'tensorflow' in locals(),\n",
    "    'MAPE_Prediccion': lstm_mape if 'lstm_mape' in locals() else None\n",
    "}\n",
    "\n",
    "pd.DataFrame([resumen_ml]).to_csv('../results/resumen_ml_rgm.csv', index=False)\n",
    "print('‚úÖ Resumen ejecutivo ML guardado')\n",
    "\n",
    "# 6. Guardar scalers importantes (usando pickle)\n",
    "import pickle\n",
    "\n",
    "scalers_to_save = {}\n",
    "if 'scaler_clv' in locals():\n",
    "    scalers_to_save['scaler_clv'] = scaler_clv\n",
    "if 'scaler_ts' in locals():\n",
    "    scalers_to_save['scaler_ts'] = scaler_ts\n",
    "\n",
    "if scalers_to_save:\n",
    "    with open('../models/scalers.pkl', 'wb') as f:\n",
    "        pickle.dump(scalers_to_save, f)\n",
    "    print('‚úÖ Scalers guardados')\n",
    "\n",
    "# 7. Guardar modelo LSTM si existe\n",
    "if 'lstm_model' in locals():\n",
    "    lstm_model.save('../models/lstm_demand_forecast.h5')\n",
    "    print('‚úÖ Modelo LSTM guardado')\n",
    "\n",
    "print('\\nüéØ RESUMEN FINAL:')\n",
    "print('=' * 30)\n",
    "print(f'‚Ä¢ Archivos generados en ../results/ y ../models/')\n",
    "print(f'‚Ä¢ Elasticidad: {len(elasticity_results) if \"elasticity_results\" in locals() else 0} productos analizados')\n",
    "print(f'‚Ä¢ Segmentaci√≥n: {int(best_model[\"N_Clusters\"]) if \"best_model\" in locals() else \"N/A\"} clusters identificados')\n",
    "print(f'‚Ä¢ CLV: Mejor modelo con R¬≤ = {best_clv_model[\"Test_R2\"]:.3f}' if 'best_clv_model' in locals() else '‚Ä¢ CLV: No calculado')\n",
    "print(f'‚Ä¢ Forecasting: MAPE = {lstm_mape:.1f}%' if 'lstm_mape' in locals() else '‚Ä¢ Forecasting: No disponible')\n",
    "print('\\n‚úÖ An√°lisis ML de RGM completado exitosamente')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
